The basic distinction, though, is that functional tests test the application from the outside, from the point of view of the user. Unit tests test the application from the inside, from the point of view of the programmer.

    We start by writing a functional test, describing the new functionality from the user’s point of view.

    Once we have a functional test that fails, we start to think about how to write code that can get it to pass (or at least to get past its current failure). We now use one or more unit tests to define how we want our code to behave—​the idea is that each line of production code we write should be tested by (at least) one of our unit tests.

    Once we have a failing unit test, we write the smallest amount of application code we can, just enough to get the unit test to pass. We may iterate between steps 2 and 3 a few times, until we think the functional test will get a little further.

    Now we can rerun our functional tests and see if they pass, or get a little further. That may prompt us to write some new unit tests, and some new code, and so on.

Functional tests should help you build an application with the right functionality, and guarantee you never accidentally break it. Unit tests should help you to write code that’s clean and bug free.



resolve is the function Django uses internally to resolve URLs and find what view function they should map to. We’re checking that resolve, when called with “/”, the root of the site, finds a function called home_page.
What function is that? It’s the view function we’re going to write next, which will actually return the HTML we want. You can see from the import that we’re planning to store it in lists/views.py.

Terminology:
Functional Test == Acceptance Test == End-to-End Test

But their point was that it’s pointless to write a comment that just repeats what you’re doing with the code:
Not only is it pointless, but there’s a danger that you’ll forget to update the comments when you update the code, and they end up being misleading. The ideal is to strive to make your code so readable, to use such good variable names and function names, and to structure it so well that you no longer need any comments to explain what the code is doing. Just a few here and there to explain why.

That’s what we call an expected fail, which is actually good news—not quite as good as a test that passes, but at least it’s failing for the right reason; we can have some confidence we’ve written the test correctly.

Tests are organised into classes, which inherit from unittest.TestCase. name starts with test is a test method, and will be run by the test runner
setUp and tearDown are special methods which get run before and after each test

Finally, we have the if __name__ == '__main__' clause (if you’ve not seen it before, that’s how a Python script checks if it’s been executed from the command line, rather than just imported by another script). We call unittest.main(), which launches the unittest test runner, which will automatically find test classes and methods in the file and run them.

TDD is a discipline, and that means it’s not something that comes naturally; because many of the payoffs aren’t immediate but only come in the longer term, you have to force yourself to do it in the moment.
I suggest following the discipline for now—​as with any discipline, you have to take the time to learn the rules before you can break them

When we hit Enter, the page will refresh. The time.sleep is there to make sure the browser has finished loading before we make any assertions about the new page. This is called an "explicit wait" (a very simple one; we’ll improve it in [chapter_explicit_waits_1]).

Big changes to a functional test are usually a good thing to commit on their own. The more atomic your commits, the better

In general, one of the rules of unit testing is Don’t test constants, and testing HTML as text is a lot like testing a constant
Unit tests are really about testing logic, flow control, and configuration
That’s a refactor—​when we try to improve the code without changing its functionality
That last bit is really important. If you try to add new functionality at the same time as refactoring, you’re much more likely to run into trouble
The first rule is that you can’t refactor without tests
they will be what makes sure that our refactoring is behaviour preserving
When refactoring, work on either the code or the tests, but not both at once. 
stick to small steps; keep refactoring and functionality changes entirely separate.

 Well, you can think of the functional test as being a high-level view of the cycle, where "writing the code" to get the functional tests to pass actually involves using another, smaller TDD cycle which uses unit tests.
 The functional tests are the ultimate judge of whether your application works or not. The unit tests are a tool to help you along the way.

the point of TDD is to allow you to do one thing at a time, when you need to



When a functional test fails with an unexpected failure, there are several things we can do to debug it:

    Add print statements, to show, for example, what the current page text is.

    Improve the error message to show more info about the current state.

    Manually visit the site yourself.

    Use time.sleep to pause the test during execution.[1]

Cross-Site Request Forgery


You should always be very worried whenever you think you’re being clever, because what you’re probably being is overcomplicated



The unit-test/code cycle is sometimes taught as Red, Green, Refactor:

    Start by writing a unit test which fails (Red).

    Write the simplest possible code to get it to pass (Green), even if that means cheating.

    Refactor to get to better code that makes more sense.

I find that leaves things a little too vague, so I usually like to use a second technique, which is called triangulation: if your tests let you get away with writing "cheating" code that you’re not happy with, like returning a magic constant, write another test that forces you to write some better code. That’s what we’re doing when we extend the FT to check that we get a "2:" when inputting a second list item.

Purists will tell you that a "real" unit test should never touch the database, and that the test I’ve just written should be more properly called an integrated test, because it doesn’t only test our code, but also relies on an external system—​that is, a database.

a long unit test either needs to be broken into two, or it may be an indication that the thing you’re testing is too complicated

Good unit testing practice says that each test should only test one thing. The reason is that it makes it easier to track down bugs. Having multiple assertions in a test means that, if the test fails on an early assertion, you don’t know what the status of the later assertions is.

Are you wondering about the line spacing in the test? I’m grouping together two lines at the beginning which set up the test, one line in the middle which actually calls the code under test, and the assertions at the end. This isn’t obligatory, but it does help see the structure of the test. Setup, Exercise, Assert is the typical structure for a unit test.
Django creates a special test database for unit tests
And we’ve used at least three different FT debugging techniques: in-line print statements, time.sleeps, and improving the error messages



Regression

    When new code breaks some aspect of the application which used to work.
Unexpected failure

    When a test fails in a way we weren’t expecting. This either means that we’ve made a mistake in our tests, or that the tests have helped us find a regression, and we need to fix something in our code.
Red/Green/Refactor

    Another way of describing the TDD process. Write a test and see it fail (Red), write some code to get it to pass (Green), then Refactor to improve the implementation.
Triangulation

    Adding a test case with a new specific example for some existing code, to justify generalising the implementation (which may be a "cheat" until that point).
Three strikes and refactor

    A rule of thumb for when to remove duplication from code. When two pieces of code look very similar, it often pays to wait until you see a third use case, so that you’re more sure about what part of the code really is the common, re-usable part to refactor out.
The scratchpad to-do list

    A place to write down things that occur to us as we’re coding, so that we can finish up what we’re doing and come back to them later.


Testing "Best Practices" Applied in this Chapter

Ensuring test isolation and managing global state

    Different tests shouldn’t affect one another. This means we need to reset any permanent state at the end of each test. Django’s test runner helps us do this by creating a test database, which it wipes clean in between each test. (See also [chapter_purist_unit_tests].)
Avoid "voodoo" sleeps

    Whenever we need to wait for something to load, it’s always tempting to throw in a quick-and-dirty time.sleep. But the problem is that the length of time we wait is always a bit of a shot in the dark, either too short and vulnerable to spurious failures, or too long and it’ll slow down our test runs. Prefer a retry loop that polls our app and moves on as soon as possible.
Don’t rely on Selenium’s implicit waits

    Selenium does theoretically do some "implicit" waits, but the implementation varies between browsers, and at the time of writing was highly unreliable in the Selenium 3 Firefox driver. "Explicit is better than implict", as the Zen of Python says, so prefer explicit waits.

critical TDD technique: how to adapt existing code using an incremental, step-by-step process which takes you from working state to working state. Testing Goat, not Refactoring Cat.
TDD is closely associated with the agile movement in software development, which includes a reaction against Big Design Up Front
But we obey another tenet of the agile gospel: "YAGNI" (pronounced yag-knee), which stands for "You ain’t gonna need it

At the top level, we’re going to use a combination of adding new functionality (by adding a new FT and writing new application code), and refactoring our application—​that is, rewriting some of the existing implementation so that it delivers the same functionality to the user but using aspects of our new design. We’ll be able to use the existing functional test to verify we don’t break what already works, and the new functional test to drive the new features
At the unit test level, we’ll be adding new tests or modifying existing ones to test for the changes we want, and we’ll be able to similarly use the unit tests we don’t touch to help make sure we don’t break anything in the process

 	I’m using the convention of double-hashes (##) to indicate "meta-comments"—comments about how the test is working and why
Being all excited about our new design, I had an overwhelming urge to dive in at this point and start changing models.py, which would have broken half the unit tests, and then pile in and change almost every single line of code, all in one go. That’s a natural urge, and TDD, as a discipline, is a constant fight against it. Obey the Testing Goat, not Refactoring Cat! We don’t need to implement our new, shiny design in a single big bang. Let’s make small changes that take us from a working state to a working state, with our design guiding us gently at each stage

There are four items on our to-do list. The FT, with its Regexp didn't match, is telling us that the second item—​giving lists their own URL and identifier—​is the one we should work on next. Let’s have a go at fixing that, and only that.
Not only is our new test failing, but the old one is too. That tells us we’ve introduced a regression. Let’s try to get back to a working state as quickly as possible by building a URL for our one and only list
Red/Green/Refactor dance